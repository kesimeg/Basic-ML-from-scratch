{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77870a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783be7a6",
   "metadata": {},
   "source": [
    "First lets load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92272cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/iris.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f14b31",
   "metadata": {},
   "source": [
    "We will represent species (or classes) as integers instead of their names. To do this we will use sci-kit's LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94a0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "data[\"species\"] = label_enc.fit_transform(data[\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af56e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d56c9c",
   "metadata": {},
   "source": [
    "Lets check how many classes we have and number of samples belonging to each class. As you can see we have a uniformly distributed data. Each class has 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c31a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([50, 50, 50]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(data[\"species\"],return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66efe72",
   "metadata": {},
   "source": [
    "Random forest classifier is an ensemble model. Ensemble models use multiple models to get better performance. Random forest classifier consists of multiple decision trees. We explain deision trees on another notebook in detail. So we will skip decision tree related parts which you can find in a seperate notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea264bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini(data):\n",
    "    _ , counts = np.unique(data,return_counts=True)\n",
    "\n",
    "    gini = (counts/len(data))**2\n",
    "\n",
    "    gini = 1 - gini.sum()\n",
    "    return gini\n",
    "\n",
    "def weighted_gini(data): # a list of arrays (two element list)\n",
    "    w_gini = []\n",
    "    n = 0\n",
    "    \n",
    "    for i in data:\n",
    "        gini = compute_gini(i)\n",
    "        w_gini.append(gini * len(i)) \n",
    "        n+=len(i)\n",
    "        \n",
    "    w_gini = np.array(w_gini)/n\n",
    "    return w_gini.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb38aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_select(X,Y): # X is data for only a single feature\n",
    "    X2 = X.copy() # we create a copy of X just for finding the thresholds (we don't want to sort the original data)\n",
    "    X2 = X2.sort_values()\n",
    " \n",
    "    thresholds = X2.rolling(2).mean()[1:] # moving average, since first value is NAN we remove it\n",
    "\n",
    "    unique_vals , _ = np.unique(thresholds,return_counts=True) # get each unique threshold value\n",
    "    \n",
    "    gini_list = []\n",
    "\n",
    "    for tresh in unique_vals: #iterate over each threshold\n",
    "        # seperate the data using threshold\n",
    "        left_d = np.where(X<tresh)[0] # actually we can iterate over thre X once and use if<thresh,else to divide data\n",
    "        right_d = np.where(X>=tresh)[0] # and not use np.where twice but numpy is faster than regular for loop\n",
    "\n",
    "        gini = weighted_gini([Y.iloc[left_d],Y.iloc[right_d]]) #select elements of Y (labels) and calculate gini\n",
    "        gini_list.append(gini)\n",
    "\n",
    "    arg_min = np.argmin(gini_list) #select lowest gini val\n",
    "    thres_val = unique_vals[arg_min] # select threshold for lowest gini\n",
    "    gini = gini_list[arg_min]\n",
    "    return thres_val,gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1928fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_feature(X,Y): # data is a pandas dataframe\n",
    "    feature_select_list = []\n",
    "\n",
    "    for feature_name in X.columns:\n",
    "        split_val,gini = gini_select(X[feature_name],Y)\n",
    "        feature_select_list.append([feature_name,gini,split_val])\n",
    "  \n",
    "    feature_select_list = np.array(feature_select_list)\n",
    "    arg_min = np.argmin(feature_select_list[:,1])\n",
    "    \n",
    "    feature_name = feature_select_list[arg_min,0]\n",
    "    threshold = feature_select_list[arg_min,2]\n",
    "    gini_val = feature_select_list[arg_min,1]\n",
    "    \n",
    "    #because we can not have np arrays with different typed\n",
    "    #elements numpy changed everything into string (due to feature name being a string) now we have to revert it\n",
    "    return feature_name,float(threshold),float(gini_val)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b44f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.label = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.feature == None:\n",
    "            return \"Leaf->Label: {}\".format(self.label)\n",
    "        else:\n",
    "            return \"Node->Feature: {} Threshold: {}\".format(self.feature,self.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9414f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesicionTree():\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def build_tree(self,node,X,Y): # data is pandas dataframe\n",
    "\n",
    "    \n",
    "        gini = compute_gini(Y)\n",
    "        if gini!=0:\n",
    "            feature_name,threshold, gini = find_split_feature(X,Y)\n",
    "\n",
    "            d = X[feature_name]\n",
    "            left_ind = np.where(d<threshold)[0]\n",
    "            right_ind = np.where(d>=threshold)[0]\n",
    "\n",
    "            if len(left_ind) !=0 and len(right_ind) != 0:\n",
    "                node.feature = feature_name\n",
    "                node.threshold = threshold\n",
    "\n",
    "                node.left = Node()\n",
    "                self.build_tree(node.left,X.iloc[left_ind,:],Y.iloc[left_ind])\n",
    "\n",
    "                node.right = Node()\n",
    "                self.build_tree(node.right,X.iloc[right_ind,:],Y.iloc[right_ind])\n",
    "            else:\n",
    "                unique_vals , counts = np.unique(Y,return_counts=True)\n",
    "                max_count = np.argmax(counts)\n",
    "                node.label = unique_vals[max_count]\n",
    "        else:\n",
    "            unique_vals , _ = np.unique(Y,return_counts=True)\n",
    "            node.label = unique_vals[0]\n",
    "            \n",
    "    def predict(self,node,data): # data is a pandas dataframe with a single sample\n",
    "        if node.label==None:\n",
    "            d = data[node.feature]\n",
    "\n",
    "            if d<node.threshold:\n",
    "\n",
    "                return self.predict(node.left,data)\n",
    "            else:\n",
    "\n",
    "                return self.predict(node.right,data)\n",
    "        else:\n",
    "\n",
    "            return node.label\n",
    "        \n",
    "    def predict_dataset(self,data): # data is a pandas dataframe\n",
    "        output = []\n",
    "        for i in range(len(data)):\n",
    "            output.append(self.predict(self.root,data.iloc[i,:]))\n",
    "        return np.array(output)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bb6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Randomforest():\n",
    "    def __init__(self,n_trees = 1):\n",
    "        \n",
    "        self.n_trees = n_trees #number of trees (must be at least 1)\n",
    "        self.trees = [DesicionTree() for i in range(self.n_trees)]\n",
    "        self.tree_features = []\n",
    "        \n",
    "    def bootstrap(self,X,Y,min_feature_num,max_feature_num):\n",
    "        indices = np.random.randint(0,len(X),len(X))\n",
    "        n_features = np.random.randint(min_feature_num,max_feature_num)\n",
    "        \n",
    "        features = np.random.choice(np.arange(len(X.columns)),n_features,replace = False) #no feature repetition\n",
    "\n",
    "        return X.iloc[indices,features],Y.iloc[indices]\n",
    "    \n",
    "    def train(self,X,Y,min_feature_num,max_feature_num): \n",
    "        for i in range(len(self.trees)):\n",
    "            X_boot, Y_boot = self.bootstrap(X,Y,min_feature_num,max_feature_num)\n",
    "            self.tree_features.append(X_boot.columns)\n",
    "            self.trees[i].build_tree(self.trees[i].root,X_boot,Y_boot)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        preds = []\n",
    "        for i in range(len(self.trees)):\n",
    "            preds.append(self.trees[i].predict(self.trees[i].root,X))\n",
    "        \n",
    "        unique_vals , counts = np.unique(preds,return_counts=True,axis=0)\n",
    "        return unique_vals[np.argmax(counts)]\n",
    "    def predict_dataset(self,X):\n",
    "        preds = []\n",
    "        for i in range(len(X)):\n",
    "            preds.append(self.predict(X.iloc[i,:]))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8e3ae",
   "metadata": {},
   "source": [
    "Now we will define our Randomforest class. As we have said before random forest consists of multiple trees. We hold these trees in a list. The idea of random forest is simple. We create multiple trees. However we don't train each tree with the same data. In this step we use a method called bootstrapping. Bootstrapping is randomly sampling the data. So we resample the data. Two important things to note are: 1) We don't use all the features in  the new version of our data. In iris dataset we have 4 features. When we use bootstrapping we can have 1 to 4 features in our new data. 2) A sample can appear more than once in the new data. We use bootstrapping and train each tree with a different resampled version of our dataset. This leads to having different trees. Now we have multiple trees but how are we going to make a prediction? We use majority voting. Each tree classifies the input sample. To give the final decision we take the most predicted class by all the trees. This whole process is called bagging (bootstrapping + aggregation). We have four methods in our Randomforest class. We will go over them one by one.  \n",
    "\n",
    "bootstrap method randomly resamples data and selects 2 to 4 features and returns bootsrapped version of data.\n",
    "\n",
    "train method is used for building our trees. We go over each tree and build it.\n",
    "\n",
    "predict method is for classification of a single sample. Since we have multiple trees we make each tree classify the input. Our final prediction is the most predicted class by all the trees.\n",
    "\n",
    "predict_dataset method takes an input with multiple samples and classfies each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d9c65",
   "metadata": {},
   "source": [
    "Now lets divide our data into train and test sets and look the performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4faef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_f = Randomforest(10) # lets have 10 trees\n",
    "random_f.train(X_train,y_train,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50fe789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = random_f.predict_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f5e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(y_test == output).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a53b5",
   "metadata": {},
   "source": [
    "You can observe that each tree uses different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "140408c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Index(['sepal_width', 'petal_length', 'petal_width'], dtype='object'),\n",
       " Index(['petal_width', 'sepal_length'], dtype='object'),\n",
       " Index(['petal_length', 'sepal_length', 'petal_width'], dtype='object'),\n",
       " Index(['petal_length', 'petal_width'], dtype='object'),\n",
       " Index(['sepal_width', 'petal_length', 'sepal_length'], dtype='object'),\n",
       " Index(['petal_width', 'sepal_width'], dtype='object'),\n",
       " Index(['petal_width', 'petal_length', 'sepal_length'], dtype='object'),\n",
       " Index(['sepal_width', 'petal_width', 'petal_length'], dtype='object'),\n",
       " Index(['petal_length', 'sepal_width'], dtype='object'),\n",
       " Index(['petal_width', 'sepal_width', 'petal_length'], dtype='object')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_f.tree_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae81f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
