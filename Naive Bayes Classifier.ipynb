{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77870a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783be7a6",
   "metadata": {},
   "source": [
    "First lets load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92272cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/iris.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f14b31",
   "metadata": {},
   "source": [
    "We will represent species (or classes) as integers instead of their names. To do this we will use sci-kit's LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94a0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "data[\"species\"] = label_enc.fit_transform(data[\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af56e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d8248",
   "metadata": {},
   "source": [
    "There are multiple variations of Naive Bayes Classifier such as Multinomial Naive Bayes Classifier or Gaussian Naive Bayes Classifier. Generally Naive Bayes Classifier name is used for mentioning Multinomial Naive Bayes Classifier. So in this tutorial we actually implement Multinomial Bayes but for short we will use the name Naive Bayes as it is generally. Naive Bayes uses categorical input. However we have ordinal data. To turn our ordinal data to categorical we will use sci-kit's Discretizer. We will uniformly divide our data into 3 bins. You can try different paramters such as changing the number of bins or using other methods of forming the bins such as k-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62ced6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "X = data.iloc[:,:-1]\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') # for k-means change strategy to kmeans\n",
    "est.fit(X)\n",
    "\n",
    "Xt = est.transform(X)\n",
    "\n",
    "Xt = Xt.astype(np.int)\n",
    "data.iloc[:,:-1] = Xt\n",
    "y = data.iloc[:,-1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7aef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0             0            1             0            0        0\n",
       "1             0            1             0            0        0\n",
       "2             0            1             0            0        0\n",
       "3             0            1             0            0        0\n",
       "4             0            2             0            0        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d56c9c",
   "metadata": {},
   "source": [
    "Lets check how many classes we have and number of samples belonging to each class. As you can see we have a uniformly distributed data. Each class has 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c31a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([50, 50, 50]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(data[\"species\"],return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f246e",
   "metadata": {},
   "source": [
    "The bayes theorem is: P(A|B) = P(B|A) * P(A) / P(B). Here P(A|B) is the probabilty of A happening given that B is true. Naive Bayes Classifier is build on this theorem. As an example lets say that A is our label and B is our features. We want to know given our features B what is the probabilty of our sample being label A. To find this we can use bayes theorem. P(A) is just the probabilty of our sample being class A. We can easily compute this from our  dataset. It will be just the frequency of samples with label A. P(B|A) is given that our label is A what is the probabily of our features being the value B. Again using our dataset we just have to compute frequency of the samples with label A and having exact feature value B. \n",
    "\n",
    "However doing this is computationaly inefficient. As the number of features and values our features can take grows the number of computations we have to do also grows larger because we have to count sampels of each case. Why this algorithm is called \"naive\" bayes is because we make an assumption that our features are indepedent. This means that we assume that the probabilty of feature #1 being a given value is not tied to feature #2 any how. With this assumption we don't have to compute frequincies of each case. We will just compute P(B|A) as P(B1,B2,B3|A) = P(B1|A) * P(B2|A) * P(B3|A) (Here B1,B2 and B3 are features). So we will just look at the probabilty of each feature being a given value and multiply them. \n",
    "\n",
    "Lastly we don't compute P(B). We skip this step because we don't actually need the exact probabilty. So we don't compute the exact probabilty but the number we compute will be proportional to the probabilty. We will compute this \"pseudo probabilty\" for each class. Our prediction will be the class with the highest probabilty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f1a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict = {}\n",
    "classes = np.unique(data[\"species\"])\n",
    "num_classes = len(classes)\n",
    "num_features = len(X.columns)\n",
    "num_samples = len(X)\n",
    "\n",
    "for specie in classes: \n",
    "    prob_dict[specie] = {}\n",
    "    prob_dict[specie][\"feature\"] = np.ones((num_classes,num_features))\n",
    "    prob_dict[specie][\"class_prob\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb884e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature': array([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       "  'class_prob': 0},\n",
       " 1: {'feature': array([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       "  'class_prob': 0},\n",
       " 2: {'feature': array([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       "  'class_prob': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021c980",
   "metadata": {},
   "source": [
    "Here we have formed a probabilty dictionary which will hold probabilities. Each key (0,1,2) of the dictionary is a class. Each class has feature and class probabilities. Class probabilty is P(A) which is number of samples with label A divided by total number of samples. Feature probabilities are shown in a matrix. In this matrix columns represent features. So there are 4 features. Rows represent the value of the features. To better understand we will give and example. Lets say that there are 2 samples with feature #1 having value 0, 1 samples of feature #2 having value 3, 0 samples with feature #3 and 5 samples with feature #4 having value 3. Lets say that all of these samples have label 0 and we don't have any other samples with label 0. Our frequenct matrix for this class will be [[2,0,0,0],[0,0,0,0],[0,1,0,5]]. Notice that there are many zero entries. When we compute probabilities there will might be features with zero probabilty. This will be a problem because when we multiply the probabilities the result will be zero. To solve this problem we just add 1 to each case meaning we will assume there is at least one sample for each case. Thats why we initialize our matrix with ones instead of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9994f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,label in zip(X.values,y.values):\n",
    "    for feature,feature_val in enumerate(x):\n",
    "        prob_dict[label][\"feature\"][feature_val,feature]+=1\n",
    "    prob_dict[label][\"class_prob\"] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f63ca3",
   "metadata": {},
   "source": [
    "Now we go all over the dataset and count the frequencies. In the outer loop we go over each sample. In the inner loop we go over each feature and increase the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2beed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature': array([[46.,  2., 51., 51.],\n",
       "         [ 6., 35.,  1.,  1.],\n",
       "         [ 1., 16.,  1.,  1.]]),\n",
       "  'class_prob': 50},\n",
       " 1: {'feature': array([[ 7., 22.,  1.,  1.],\n",
       "         [39., 30., 49., 49.],\n",
       "         [ 7.,  1.,  3.,  3.]]),\n",
       "  'class_prob': 50},\n",
       " 2: {'feature': array([[ 2., 12.,  1.,  1.],\n",
       "         [28., 37.,  7.,  5.],\n",
       "         [23.,  4., 45., 47.]]),\n",
       "  'class_prob': 50}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db33d8",
   "metadata": {},
   "source": [
    "Lets look at feature matrix of class 0. From this matrix we can say that there are 45 examples with feature #1 having value 0. Feature #3 and #4 is always 0 in this class. You can see that because number of samples having these value is equal to total number of samples with label 0. (Don't forget that we added 1 to solve our zero probabilty problem). Now to compute probabilties we will divide our feature matrix with class_prob which is currently the number of samples belonging to that class. We will also compute P(A) that is class_prob. To calculate it we just have to divide class_prob with the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceab5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in prob_dict.keys():\n",
    "   \n",
    "    prob_dict[i][\"feature\"]/= (prob_dict[i][\"class_prob\"]+1) # +1 is because of the 1 we added in the beginning\n",
    "    \n",
    "    prob_dict[i][\"class_prob\"]/= num_samples\n",
    "    prob_dict[i][\"class_prob\"] = prob_dict[i][\"class_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ca02e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature': array([[0.90196078, 0.03921569, 1.        , 1.        ],\n",
       "         [0.11764706, 0.68627451, 0.01960784, 0.01960784],\n",
       "         [0.01960784, 0.31372549, 0.01960784, 0.01960784]]),\n",
       "  'class_prob': 0.3333333333333333},\n",
       " 1: {'feature': array([[0.1372549 , 0.43137255, 0.01960784, 0.01960784],\n",
       "         [0.76470588, 0.58823529, 0.96078431, 0.96078431],\n",
       "         [0.1372549 , 0.01960784, 0.05882353, 0.05882353]]),\n",
       "  'class_prob': 0.3333333333333333},\n",
       " 2: {'feature': array([[0.03921569, 0.23529412, 0.01960784, 0.01960784],\n",
       "         [0.54901961, 0.7254902 , 0.1372549 , 0.09803922],\n",
       "         [0.45098039, 0.07843137, 0.88235294, 0.92156863]]),\n",
       "  'class_prob': 0.3333333333333333}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb1cc0",
   "metadata": {},
   "source": [
    "Now we can do predictions. As we said in the beginning we will multiply each probabilty. We will initilaize a probabilty matrix and write probabilities into this matrix. Here prob_dict[j][\"feature\"][X.iloc[i,:].values,np.arange(num_features)] will select each probabilty depending on the each value of the samples features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00e9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prob_matrix = np.zeros((num_samples,num_classes))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    spec = y[i]\n",
    "    for j in classes:\n",
    "        prob = prob_dict[j][\"class_prob\"] * prob_dict[j][\"feature\"][X.iloc[i,:].values,np.arange(num_features)].prod()\n",
    "        prob_matrix[i,j] = prob\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a13768",
   "metadata": {},
   "source": [
    "Lets look at the first 10 examples probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe712df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [9.43226964e-02, 3.44902292e-07, 3.94174048e-07],\n",
       "       [9.43226964e-02, 3.44902292e-07, 3.94174048e-07],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06],\n",
       "       [2.06330898e-01, 1.03470688e-05, 3.64610995e-06]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdedbf",
   "metadata": {},
   "source": [
    "As you can see our probabilities don't make up to 1. This is because we are not calculating exact probabilities as we have said before. To find the label we will take the argument with the highest value using argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc2dc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_max = np.argmax(prob_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "583596d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c1e05",
   "metadata": {},
   "source": [
    "Now lets use sci-kit's Naive Bayes Classifier and compare with our implementations results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e3b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f91adc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(Xt, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4241dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99992164e-01, 3.65659171e-06, 4.17896196e-06],\n",
       "       [9.99992164e-01, 3.65659171e-06, 4.17896196e-06],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05],\n",
       "       [9.99932185e-01, 5.01445357e-05, 1.76699792e-05]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(Xt)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d2aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(Xt)\n",
    "pred = pred.astype(np.int)\n",
    "print(pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dbcc5",
   "metadata": {},
   "source": [
    "Lets check what is the percentage of our predictions which are same with sci-kit's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa1574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(pred == arg_max).sum()/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8cc2119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6a5d646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e56c1",
   "metadata": {},
   "source": [
    "Lets also check our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "532aa618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(y.values == pred).sum()/num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb91cb",
   "metadata": {},
   "source": [
    "We have %96 accuracy. Note that we did not divide our data into test,train sets as we should. We skipped these steps because we only wanted to implement the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
